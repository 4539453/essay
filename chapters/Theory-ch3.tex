\section{Алгоритмы классического машинного обучения}

\subsection{Логистическая регрессия}
\label{subsection:logreg}

Логистическая регрессия --- классическая дискриминативной линейная модель классификации. Дискриминативная значит, что нас интересует $P (y = k | x)$, а не совместное распределение $p (x, y)$. Свое начало она берет из расстояния Кульбака-Лейблера. Оно задается формулой:

\begin{equation}
 KL(P||Q) = \int\log\frac{dP}{dQ}dP,
\end{equation}

, где $P$ --- истинное распределение, а $Q$ --- приближенное. Для дискретного случая:

\begin{equation}
 KL(P||Q) = \sum_{y} p(y)log\frac{p(y)}{q(y)},
\end{equation}

а если раскрыть получаем:

\begin{equation}
\begin{aligned}
 KL(P||Q) & = \sum_y p(y)\log\frac{p(y)}{q(y)} = \\
 & = \sum_y p(y) \log p(y) - \sum_y p(y) \log q(y) = - H(p) + H(p,q),
\end{aligned}
\end{equation}

, где $H(p)$ ---  энтропия распределения $p$, a $H(p, q)$ --- наша кросс энтропия. Из этой суммы видно, что нам нужно минимизировать $H(p, q)$. Для бинарной классификации loss-функция будет выглядеть так:

\begin{equation} \label{eq:logLoss}
 L(w)= H(p_{data}, q(w)) = -\frac{1}{N}\sum_{i=1}^N(y_i\log\hat y_i(w) + (1-y_i)\log(1-\hat y_i(w)))
\end{equation}

где $p_{data}$ --- распределение наших данных, $q(w)$ --- апостериорное распределение,  $\hat y_i(w)$ --- оценка вероятности при входных параметрах $w$ и $y_i$ --- истинное предсказание. Два слагаемых мы получаем, т.к. события несовместные. Например, в тексте говорится о кошечках или о собачках, события появления кошечки или собачки несовместные, т.е. $p(\text{кошечки}) = 1 - p(\text{собачки})$. Если мы предсказываем кошечку (1), как абсолютный 0 или собачку (0), как 1, то ошибка будет бесконечной из первого и второго слагаемых соответственно -- это не допустимо.

Перед тем, как перейти к нескольким классам, рассмотрим сначала задачу классификации с Байейсовской точки зрения: определим для каждого класса $C_k$ плотность $p(x|C_k)$ и какие-то априорные распределения $p(C_k)$ (пускай это будут размеры классов, т.е. мы ничего не знаем о примере, но предполагаем с какой вероятностью он относится к конкретному классу) и найдем $p(C_k|x)$. Для двух классов:

\begin{equation}
\begin{aligned}
 p(C_1|x) & = \frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)} = \\
 & = 1/\frac{(p(x|C_1)p(C_1)+p(x|C_2)p(C_2))}{p(x|C_1)p(C_1)} = \\
 & = 1/(1+\frac{p(x|C_2)p(C_2)}{p(x|C_1)p(C_1)}) = \frac{1}{1+e^{-a}} = \sigma (a),
\end{aligned}
\end{equation}

где

\begin{equation}
 a = \ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)},\qquad \frac{1}{1+e^{-a}} = \sigma (a).
\end{equation}

Используя логистическую регрессию мы делаем предположение о виде аргумента сигдмоиды $a$ --- это будет скалярное произведение вектора признаков на вектор данных: $a = w_\top x$. Cигмоида переводит результат вычисления этой линейной функции на отрезок $[0;1]$ и как результат мы получаем апостериорную вероятность первого или второго классов:

\begin{equation}
 p(C_1|x) = y(x) = \sigma (w_\top x),\qquad p(C_2|x) = 1-p(C_1|x),
\end{equation}

чтобы обучить эту модель мы можем просто оптимизировать правдоподобие по $w$.

Для набора ${x_n, t_n}$, где $x_n$ -- входы, а $t_n \in \{0;1\}$ -- соответствующие метки классов, получается такое правдоподобие:

\begin{equation}
 p(t|w) = \prod_{n=1}^N y_n^{t_n}(1-y_n)^{1-t_n},\quad\text{где}\quad y_n = p(C_1|x_n).
\end{equation}

И теперь мы, максимизируя логарифм вероятности, ищем наилучшие параметры функции правдоподобия для этого можно использовать различные оптимизаторы.

\begin{equation}
 \lh (w) = -\ln p(t|w) = -\sum_{n=1}^N[t_n \ln y_n + (1-t_n) \ln (1-y_n)].
\end{equation}

Теперь можно легко обобщить задачу на несколько классов. Только вместо сигмоиды будем использовать softmax функцию. Для $K$ классов получаем:

\begin{equation}
 p(C_k|x) = \frac{p(x|C_k)p(C_k)}{\sum_{j=1}^K p(x|C_j)p(C_j)} = \frac{e^{a_k}}{\sum_{j=1}^K e^{a_j}},
\end{equation}

где количество аргументов $a_k = \ln p(x|C_k)p(C_k)$ равняется количеству классов. Функция правдоподобия почти не изменилась. Пусть на вход метки класса подаются в формате one-hot векторов, тогда для набора векторов $T = {t_n}$ функция правдоподобия выглядит следующим образом:

\begin{equation}
 p(T|w_1,\ldots,w_K) = \prod_{n=1}^N \prod_{k=1}^K p(C_k|x_n)^{t_{nk}} = \prod_{n=1}^N \prod_{k=1}^K y_{nk}^{t_{nk}}.
\end{equation}

, где $y_{nk} = y_k(x_n)$. Опять переходим к логарифму и получаем функцию максимального правдоподобия для $K$ классов:

\begin{equation}
 \lh (w_1,\ldots,w_K) = -\ln p(T|w_1,\ldots,w_K) = \sum_{n=1}^N \sum_{k=1}^K t_{nk}\ln y_{nk}.
\end{equation}


\subsection{Метод опорных ввекторов}





























