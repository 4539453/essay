\subsection{Классификация}
\vspace{-1.3cm}

\subsubsection{Анализ качества классификации}

Для оценки качества классификации используется Macro\,F1-мера --- нормализованное по всем классам среднее гармоническое метрик $P$ и $R$:

    \begin{equation*}
    \text{Macro F1} =  \frac{1}{n}\sum_{i=1}^n 2\frac{P_i\times R_i}{P_i + R_i} = \frac{1}{n}\sum_{i=1}^n
\text{F1}_i,
    \end{equation*}

где $P$ --- точность и $R$ --- полнота.

\bigskip\noindent
Ее преймущество в том, что она не учитывает дисбаланс классов.

\begin{definition}
 Точность (\textit{precision}) --- показывает долю объектов, которые классификатор определил, как принадлежащие действительно правильному классу.
\end{definition}

\begin{equation*}
 P = \frac{TP}{TP + FP},
\end{equation*}

\bigskip
где $TP$ --- те метки которые мы ожидали и получили, $FP$ --- те метки которые мы не ожидали, но получили на выходе.

\begin{definition}
 Полнота (recall) --- показывает какую долю объектов из всего множества конкретного класса классификатор определил верно.
\end{definition}

\begin{equation*}
 R = \frac{TP}{TP + FN},
\end{equation*}

\bigskip
где $FN$ --- те метки которые мы ожидали, но не получили на выходе.




\subsubsection{Кросс-валидация}

Методом оценки модели и ее поведения на тестовых данных является стратифицированная перекрестная проверка stratified k-fold cross validation, при $K$ = 10.

Пусть $X$ --- множество описаний документов, $Y$ --- множество возможных меток.
Есть конечная выборка документов $X^L = (x_i,y_i)_{i=1}^L \subset X\times Y$.
Задан алгоритм обучения --- отображение $f$, которое произвольной конечной выборке $X^m$ ставит в соответствие функцию --- алгоритм $f:\,X\to Y$.

\bigskip
Качество алгоритма a оценивается по произвольной выборке документов $X^m$ с помощью функционала качества $Q(f,X^m)$. Для перекрестной проверки не важно, как именно вычисляется этот функционал:

\begin{equation*}
 Q(f,X^m)=\frac{1}{m}\sum_{x_i\in X^m} L(f(x_i),y_i),
\end{equation*}

\bigskip
где $L(f(x_i),y_i)$ — неотрицательная функция потерь (loss), возвращающая величину ошибки классификатора $f(x_i)$ при правильном ответе $y_i$.

\bigskip
Выборка $X^L$ разбивается $K$ способами на две подвыборки: $X^L = X^m_k \cup X^n_k$, где $X^m_n$ — обучающая подвыборка длины $m$, $X^n_k$ — контрольная подвыборка длины $k=L-m$, $k=1,\ldots,K$ — номер разбиения.

\bigskip
Для каждого разбиения $K$ строится алгоритм $f_n = \mu(X^m_n)$ и вычисляется значение функционала качества $Q_k = Q (f_k, X^n_k)$. Нормализованное значение $Q_k$ по всем разбиениям называется перекрестной проверкой:

\begin{equation*}
 CV(\mu,X^L)=\frac1K \sum_{k=1}^K Q (\mu(X^m_k), X^n_k).
\end{equation*}

\bigskip
Стратифицированная выборка позволяет сохранить процентное содержание документов для каждого класса.




















\subsubsection{Логистическая регрессия}
\label{subsection:logreg}

Логистическая регрессия --- классическая дискриминативной линейная модель классификации. Дискриминативная
значит, что нас интересует $P (y = k | x)$, а не совместное распределение $p (x, y)$. Свое начало она берет из
расстояния Кульбака-Лейблера. Оно задается формулой:

\begin{equation}
 KL(P||Q) = \int\ln\frac{dP}{dQ}dP,
\end{equation}

, где $P$ --- истинное распределение, а $Q$ --- приближенное. Для дискретного случая:

\begin{equation}
 KL(P||Q) = \sum_{y} p(y)log\frac{p(y)}{q(y)},
\end{equation}

а если раскрыть получаем:

\begin{equation}
\begin{aligned}
 KL(P||Q) & = \sum_y p(y)\ln\frac{p(y)}{q(y)} = \\
 & = \sum_y p(y) \ln p(y) - \sum_y p(y) \ln q(y) = - H(p) + H(p,q),
\end{aligned}
\end{equation}

, где $H(p)$ ---  энтропия распределения $p$, a $H(p, q)$ --- наша кросс энтропия. Из этой суммы видно, что
нам нужно минимизировать $H(p, q)$. Для бинарной классификации loss-функция будет выглядеть так:

\begin{equation} \label{eq:logLoss}
 L(w)= H(p_{data}, q(w)) = -\frac{1}{N}\sum_{i=1}^N(y_i\ln\hat y_i(w) + (1-y_i)\ln(1-\hat y_i(w)))
\end{equation}

где $p_{data}$ --- распределение наших данных, $q(w)$ --- апостериорное распределение,  $\hat y_i(w)$ ---
оценка вероятности при входных параметрах $w$ и $y_i$ --- истинное предсказание. Два слагаемых мы получаем,
т.к. события несовместные. Например, в тексте говорится о кошечках или о собачках, события появления кошечки
или собачки несовместные, т.е. $p(\text{кошечки}) = 1 - p(\text{собачки})$. Если мы предсказываем кошечку (1),
как абсолютный 0 или собачку (0), как 1, то ошибка будет бесконечной из первого и второго слагаемых
соответственно -- это не допустимо.

Перед тем, как перейти к нескольким классам, рассмотрим сначала задачу классификации с Байейсовской точки
зрения: определим для каждого класса $C_k$ плотность $p(x|C_k)$ и какие-то априорные распределения $p(C_k)$
(пускай это будут размеры классов, т.е. мы ничего не знаем о примере, но предполагаем с какой вероятностью он
относится к конкретному классу) и найдем $p(C_k|x)$. Для двух классов:

\begin{equation}
\begin{aligned}
 p(C_1|x) & = \frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)} = \\
 & = 1/\frac{(p(x|C_1)p(C_1)+p(x|C_2)p(C_2))}{p(x|C_1)p(C_1)} = \\
 & = 1/(1+\frac{p(x|C_2)p(C_2)}{p(x|C_1)p(C_1)}) = \frac{1}{1+e^{-a}} = \sigma (a),
\end{aligned}
\end{equation}

где

\begin{equation}
 a = \ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)},\qquad \frac{1}{1+e^{-a}} = \sigma (a).
\end{equation}

Используя логистическую регрессию мы делаем предположение о виде аргумента сигдмоиды $a$ --- это будет
скалярное произведение вектора признаков на вектор данных: $a = w_\top x$. Cигмоида переводит результат
вычисления этой линейной функции на отрезок $[0;1]$ и как результат мы получаем апостериорную вероятность
первого или второго классов:

\begin{equation}
 p(C_1|x) = y(x) = \sigma (w_\top x),\qquad p(C_2|x) = 1-p(C_1|x),
\end{equation}

чтобы обучить эту модель мы можем просто оптимизировать правдоподобие по $w$.

Для набора ${x_n, t_n}$, где $x_n$ -- входы, а $t_n \in \{0;1\}$ -- соответствующие метки классов, получается
такое правдоподобие:

\begin{equation}
 p(t|w) = \prod_{n=1}^N y_n^{t_n}(1-y_n)^{1-t_n},\quad\text{где}\quad y_n = p(C_1|x_n).
\end{equation}

И теперь мы, максимизируя логарифм вероятности, ищем наилучшие параметры функции правдоподобия для этого можно
использовать различные оптимизаторы.

\begin{equation}
 \lh (w) = -\ln p(t|w) = -\sum_{n=1}^N[t_n \ln y_n + (1-t_n) \ln (1-y_n)].
\end{equation}

Теперь можно легко обобщить задачу на несколько классов. Только вместо сигмоиды будем использовать softmax
функцию. Для $K$ классов получаем:

\begin{equation}
 p(C_k|x) = \frac{p(x|C_k)p(C_k)}{\sum_{j=1}^K p(x|C_j)p(C_j)} = \frac{e^{a_k}}{\sum_{j=1}^K e^{a_j}},
\end{equation}

где количество аргументов $a_k = \ln p(x|C_k)p(C_k)$ равняется количеству классов. Функция правдоподобия почти
не изменилась. Пусть на вход метки класса подаются в формате one-hot векторов, тогда для набора векторов $T =
{t_n}$ функция правдоподобия выглядит следующим образом:

\begin{equation}
 p(T|w_1,\ldots,w_K) = \prod_{n=1}^N \prod_{k=1}^K p(C_k|x_n)^{t_{nk}} = \prod_{n=1}^N \prod_{k=1}^K
y_{nk}^{t_{nk}}.
\end{equation}

, где $y_{nk} = y_k(x_n)$. Опять переходим к логарифму и получаем функцию максимального правдоподобия для $K$
классов:

\begin{equation}
 \lh (w_1,\ldots,w_K) = -\ln p(T|w_1,\ldots,w_K) = \sum_{n=1}^N \sum_{k=1}^K t_{nk}\ln y_{nk}.
\end{equation}


\subsubsection{Метод опорных векторов}

Главная задача SVM сводится к нахождению разделяющей гиперплоскости в пространстве $\mathbb{R}^n$, то есть этот алгоритм решает задачу бинарной классификации, где объектам из $X \in \mathbb{R}^n$ ставится в соответствие один из двух классов $Y = \{+1, -1\}$.

\bigskip
Чтобы перейти к задаче многоклассовой классификации используется подход one-to-rest, суть что мы используем столько же SWM классификаторов, сколько классов. И классы просто сравниваются между собой.

\bigskip
Но вернемся к объяснению алгоритма. Чтобы найти такую гиперплоскость выборка должна быть линейно разделимой, что на практике встречается очень редко. Обычно данные содержат шум и нечеткие границы между классами. В таком пространстве стандартная задача нахождения гиперплоскости неразрешима. Обойти это ограничение нам поможет замысловатый трюк.

\bigskip
Пусть поставлена задача нелинейного программирования с ограничениями:

$$
\begin{cases}
f(x) \to \min\limits_{x \in X} \\
g_i(x) \leq 0,\;i=1\ldots m \\
h_j(x) = 0,\;j=1\ldots k
\end{cases}
$$

\bigskip
Если $x$ — точка локального минимума при наложенных ограничениях, то существуют такие множители $\mu_i, i = 1\ldots m$, $\;\lambda_j, j = 1\ldots k$, что для функции Лагранжа $L(x; \mu, \lambda)$ выполняются условия:


$$\begin{cases}\frac{\partial L}{\partial x} = 0, \quad L(x; \mu, \lambda) = f(x) + \sum\limits_{i=1}^m \mu_i g_i(x) + \sum\limits_{j=1}^k \lambda_j h_j(x) \\ g_i(x) \leq 0,\;h_j(x) = 0 \quad \text{(исходные ограничения)} \\ \mu_i \geq 0 \quad \text{(двойственные ограничения)} \\ \mu_i g_i(x) = 0 \quad \text{(условие дополняющей нежёсткости)} \end{cases}$$

\bigskip
При этом искомая точка является седловой точкой функции Лагранжа: минимумом по $x$ и максимумом по двойственным переменным $\mu$.

\bigskip
По теореме Каруша—Куна—Таккера, поставленная нами задача минимизации эквивалентна двойственной задаче поиска седловой точки функции Лагранжа:

$$L(\vec{w},b,\xi; \lambda, \eta) = \frac{1}{2} \lVert w \rVert^2 - \sum\limits_{i=1}^\ell \lambda_i \left(M_i(\vec{w}, b) - 1\right) - \sum\limits_{i=1}^\ell \xi_i \left(\lambda_i + \eta_i - C\right)$$

\bigskip
$\lambda_i$ — переменные, двойственные к ограничениям $M_i \geq 1 - \xi_i$, $\eta_i$ — переменные, двойственные к ограничениям $\xi_i \geq 0$

\bigskip
Запишем необходимые условия седловой точки функции Лагранжа:

\bigskip
\begin{equation}
 \begin{cases}
\frac{\partial L}{\partial w} = 0, \quad \frac{\partial L}{\partial b} = 0, \quad \frac{\partial L}{\partial \xi} = 0 \\
\xi_i \geq 0, \quad \lambda_i \geq 0, \quad \eta_i \geq 0, \quad i = 1, \ldots, \ell \\
\lambda_i = 0 \;\text{либо}\; M_i(\vec{w},b) = 1 - \xi_i, \quad i = 1, \ldots, \ell \\
\eta_i = 0 \;\text{либо}\; \xi_i = 0,\quad i = 1, \ldots, \ell
\end{cases}
\end{equation}

\bigskip
Продифференцируем функцию Лагранжа и приравняем к нулю производные. Получим следующие ограничения:

$$\begin{array}{lcl}
\frac{\partial L}{\partial w} = \vec{w} - \sum\limits_{i=1}^\ell \lambda_i y_i \vec{x}_i = 0 & \Rightarrow & \vec{w} = \sum\limits_{i=1}^\ell \lambda_i y_i \vec{x}_i \\
\frac{\partial L}{\partial b} = -\sum\limits_{i=1}^\ell \lambda_i y_i = 0 & \Rightarrow & \sum\limits_{i=1}^\ell \lambda_i y_i = 0 \\
\frac{\partial L}{\partial \xi_i} = -\lambda_i - \eta_i + C = 0 & \Rightarrow & \eta_i + \lambda_i = C, \quad i = 1, \ldots, \ell
\end{array}$$

\bigskip
Заметим, что $\eta_i \geq 0$, $\lambda_i \geq 0$, $C > 0$, поэтому из последнего ограничения получаем $0 \leq \eta_i \leq C$, $0 \leq \lambda_i \leq C$.

\bigskip
Диапазон значений $\lambda_i$ (которые, как указано выше, соответствуют ограничениям на величину отступа) позволяет нам разделить объекты обучающей выборки на три типа:

\bigskip
\begin{itemize}
 \item $\lambda_i = 0 \; \Rightarrow \; \eta_i = C; \; \xi_i = 0; \; M_i \geq 1 \;$ — периферийные (неинформативные) объекты Эти объекты лежат в своём классе, классифицируются верно и не влияют на выбор разделяющей гиперплоскости;

 \item $0 < \lambda_i < C \; \Rightarrow \; 0 < \eta_i < C; \; \xi_i = 0; \; M_i = 1 \;$ — опорные граничные объекты Эти объекты лежат ровно на границе разделяющей полосы на стороне своего класса

 \item $\lambda_i = C \; \Rightarrow \; \eta_i = 0; \; \xi_i > 0; \; M_i < 1 \;$ — опорные объекты-нарушители Эти объекты лежат внутри разделяющей полосы или на стороне чужого класса
\end{itemize}

\begin{definition}
 Опорный вектор — объект $\vec{x}_i$, соответствующий которому множитель Лагранжа отличен от нуля: $\lambda_i \neq 0$.
\end{definition}

\bigskip
Теперь для решения проблемы линейной разделимости применим трюк с ядром (kernel trick) и подставим ограничения, которые мы получили при дифференцировании, в функцию Лагранжа. Получим следующую постановку двойственной задачи:

\begin{equation*}
\begin{cases}
    -L(\lambda) = -\sum\limits_{i=1}^\ell \lambda_i + \frac{1}{2} \sum\limits_{i=1}^\ell \sum\limits_{j=1}^\ell \lambda_i \lambda_j y_i y_j K(\vec{x}_i, \vec{x}_j) \to \min\limits_\lambda \\
    0 \leq \lambda_i \leq C, \quad i = 1, \ldots, \ell \\
    \sum\limits_{i=1}^\ell \lambda_i y_i = 0
\end{cases}
\end{equation*}

Суть, что если выборка объектов с признаковым описанием из $X = \mathbb{R}^n$ не является линейно разделимой, мы можем предположить, что существует некоторое пространство $H$, вероятно, большей размерности, при переходе в которое выборка станет линейно разделимой. Пространство $H$ здесь называют спрямляющим, а функцию перехода $\psi : X \to H$ — спрямляющим отображением. Построение SVM в таком случае происходит так же, как и раньше, но в качестве векторов признаковых описаний используются векторы $\psi(\vec{x})$, а не $\vec{x}$. Соответственно, скалярное произведение $\langle \vec{x}_1, \vec{x}_2 \rangle$ в пространстве $X$ везде заменяется скалярным произведением $\langle \psi(\vec{x}_1), \psi(\vec{x}_2) \rangle$ в пространстве $H$. Отсюда следует, что пространство $H$ должно быть гильбертовым, так как в нём должно быть определено скалярное произведение.

\bigskip
Нелинейный классификатор с ядром $K$:

$$f(x) = sign \left(\sum\limits_{i=1}^\ell \lambda_i y_i K(\vec{x}_i, \vec{x})\color{black} - b\right)$$

\bigskip
Алгоритмы для нахождения решения: SMO, INCAS. Они хороши тем, что учитывают особенности применяемой SVM архитектуры.

% Обратим внимание на то, что постановка задачи и алгоритм классификации не используют в явном виде признаковое описание и оперируют только скалярными произведениями признаков объектов. Это даёт возможность заменить скалярное произведение в пространстве $X$ на [[Ядра|ядро]] — функцию, являющуюся скалярным произведением в некотором $H$. При этом можно вообще не строить спрямляющее пространство в явном виде, и вместо подбора $\psi$ подбирать непосредственно ядро.













\subsubsection{Случайный лес}

Случайный лес — один из примеров объединения классификаторов в ансамбль.


\bigskip
Итоговый классификатор — $a(x) = \frac{1}{N} \sum_{i = 1}^{N} t_i(x)$. Для задачи классификации мы выбираем решение по большинству результатов, выданных классификаторами, а в задаче регрессии — по их среднему значению. Таким образом, случайный лес — бэггинг над решающими деревьями, при обучении которых для каждого разбиения признаки выбираются из некоторого случайного подмножества признаков.


\bigskip
В качестве классификаторов используется алгоритм CART (Classification and Regression Tree) --- это алгоритм обучения деревьев решений, позволяющий использовать как дискретную, так и непрерывную целевую переменную, то есть решать как задачи классификации, так и регрессии. Алгоритм строит деревья, которые в каждом узле имеют только два потомка.

\bigskip
Качество разбиения оценивается по следующей формуле:

\begin{equation*}
 Gini_{split}  = \frac{L}{N} \left( 1-\sum_{i=1}^n\left(\frac{l_i}{L}\right)^2 \right) + \frac{R}{N} \left(1- \sum_{i=1}^n\left(\frac{r_i}{R}\right)^2 \right) \to \min,
\end{equation*}


\bigskip
где $N$ -- число примеров в узле -- предке, $L$ и $R$ число экземпляров $i$-го класса в левом/правом потомке.

























