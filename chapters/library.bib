Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1162},
isbn = {9781937284961},
pages = {1532--1543},
publisher = {Association for Computational Linguistics (ACL)},
title = {{GloVe: Global vectors for word representation}},
url = {http://nlp.},
year = {2014}
}
@book{Nikolenko,
abstract = {Перед вами — первая книга о глубоком обучении, написанная на русском языке. Глубокие модели оказались ключом, который подходит ко всем замкам сразу: новые архитектуры и алгоритмы обучения, а также увеличившиеся вычислительные мощности и появившиеся огромные наборы данных привели к революционным прорывам в компьютерном зрении, распознавании речи, обработке естественного языка и многих других типично «человеческих» задачах машинного обучения. Эти захватывающие идеи, вся история и основные компоненты революции глубокого обучения, а также самые современные достижения этой области доступно и интересно изложены в книге. Максимум объяснений, минимум кода, серьезный материал о машинном обучении и увлекательное изложение — в этой уникальной работе замечательных российских ученых и интеллектуалов.},
author = {Николенко, Сергей Игоревич and Кадурин, А.А. and Архангельская, E.О.},
booktitle = {Библиотека Программиста},
file = {:home/dl2718/dl2718/dataScienceLit/textbooks/8. Глубокое обучение. Погружение в мир нейронных сетей.pdf:pdf},
isbn = {9785496025362},
pages = {480},
title = {{Глубокое Обучение. Погружение В Мир Нейронных Сетей}},
volume = {91},
year = {2018}
}
@article{Semina,
author = {Semina, T. A.},
file = {:home/dl2718/dl2718/dataScienceLit/0-essay/1. Semina.pdf:pdf},
journal = {Социальные и гуманитарные науки. Отечественная и зарубежная литература. Серия 6: Языкознание. Реферативный журнал},
pages = {47--64},
title = {{Sentiment analysis: Modern approaches and existing problems.}},
year = {2020}
}
@misc{LenaVoita,
author = {{Lena Voita}},
keywords = {ANTI-PLAGIARISM,AUTHOR ATTRIBUTION,DISTRIBUTIONAL SEMANTICS,NATURAL LANGUAGE PROGRAMMING,SOURCE RETRIEVAL,UNKNOWN AUTHOR,АНТИПЛАГИАТ,АТРИБУЦИЯ АВТОРСТВА,ДИСТРИБУТИВНАЯ СЕМАНТИКА,НЕИЗВЕСТНЫЙ АВТОР,ПОИСК ИСТОЧНИКА,ПРОГРАММИРОВАНИЕ НА ЕСТЕСТВЕННОМ ЯЗЫКЕ},
title = {{Word Embeddings}},
url = {https://lena-voita.github.io/nlp_course/word_embeddings.html},
urldate = {2021-05-18}
}
@article{Mikolov:2,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1310.4546},
file = {:home/dl2718/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {oct},
publisher = {Neural information processing systems foundation},
title = {{Distributed representations of words and phrases and their compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}
@inproceedings{Zhou,
abstract = {Neural probabilistic parsers are attractive for their capability of automatic feature combination and small data sizes. A transition-based greedy neural parser has given better accuracies over its linear counterpart. We propose a neural probabilistic structured-prediction model for transition-based dependency parsing, which integrates search and learning. Beam search is used for decoding, and contrastive learning is performed for maximizing the sentence-level log-likelihood. In standard Penn Treebank experiments, the structured neural parser achieves a 1.8% accuracy improvement upon a competitive greedy neural parser baseline, giving performance comparable to the best linear parser.},
author = {Zhou, Hao and Zhang, Yue and Huang, Shujian and Chen, Jiajun},
booktitle = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/p15-1117},
file = {:home/dl2718/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2015 - A neural probabilistic structured-prediction model for transition-based dependency parsing.pdf:pdf},
isbn = {9781941643723},
pages = {1213--1222},
publisher = {Association for Computational Linguistics (ACL)},
title = {{A neural probabilistic structured-prediction model for transition-based dependency parsing}},
url = {https://www.aclweb.org/anthology/P15-1117},
volume = {1},
year = {2015}
}
@techreport{Goldberg,
abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
archivePrefix = {arXiv},
arxivId = {1402.3722},
author = {Goldberg, Yoav and Levy, Omer},
eprint = {1402.3722},
file = {:home/dl2718/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberg et al. - 2014 - ().pdf:pdf},
keywords = {()},
title = {{word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method}},
url = {http://arxiv.org/abs/1402.3722},
year = {2014}
}
@inproceedings{Mikolov:1,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
eprint = {1301.3781},
file = {:home/dl2718/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient estimation of word representations in vector space.pdf:pdf},
month = {jan},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Efficient estimation of word representations in vector space}},
url = {http://ronan.collobert.com/senna/},
year = {2013}
}
@techreport{Bengio,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/dl2718/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
isbn = {0262122413},
issn = {10495258},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation},
pages = {1137--1155},
title = {{A neural probabilistic language model}},
volume = {3},
year = {2001}
}
